{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5029488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3502.250244140625\n",
      "199 1666.557861328125\n",
      "299 1427.4190673828125\n",
      "399 1292.9361572265625\n",
      "499 1131.565673828125\n",
      "599 958.1602783203125\n",
      "699 790.1309814453125\n",
      "799 636.689453125\n",
      "899 500.6023864746094\n",
      "999 382.4841003417969\n",
      "1099 282.303955078125\n",
      "1199 199.62237548828125\n",
      "1299 133.82302856445312\n",
      "1399 83.97315979003906\n",
      "1499 48.941253662109375\n",
      "1599 26.49860954284668\n",
      "1699 14.60747241973877\n",
      "1799 10.011432647705078\n",
      "1899 9.011791229248047\n",
      "1999 8.927379608154297\n",
      "Result: y = 0.0005028784507885575 + 0.8550434112548828 x + 0.0005029808962717652 x^2 + -0.09366913884878159 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "574dceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(2, 50)\n",
    "        self.linear2 = torch.nn.Linear(50, 16)\n",
    "        self.linear3 = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer1_out = F.relu(self.linear1(x))\n",
    "        layer2_out = F.relu(self.linear2(layer1_out))\n",
    "        out = self.linear3(layer2_out)\n",
    "        return out, layer1_out, layer2_out\n",
    "\n",
    "batchsize = 1000\n",
    "lambda1, lambda2 = 0.1, 0.1\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "# usually following code is looped over all batches \n",
    "# but let's just do a dummy batch for brevity\n",
    "\n",
    "inputs = torch.rand(1000).reshape(-1,2)\n",
    "targets = inputs.sum(axis=1).reshape(-1,1)\n",
    "mse=[]\n",
    "for t in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, layer1_out, layer2_out = model(inputs)\n",
    "    mse_loss = loss_fn(outputs, targets)\n",
    "\n",
    "    all_linear1_params = torch.cat([x.view(-1) for x in model.linear1.parameters()])\n",
    "    all_linear2_params = torch.cat([x.view(-1) for x in model.linear2.parameters()])\n",
    "\n",
    "    l1_regularization = lambda1 * torch.norm(all_linear1_params, 1)\n",
    "    l2_regularization = lambda2 * torch.norm(all_linear2_params, 2)\n",
    "    mse.append(mse_loss)\n",
    "    loss = mse_loss + l1_regularization + l2_regularization\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eacf9ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0593,  0.1378,  0.0661, -0.1313,  0.0464, -0.0451, -0.0467,  0.0934,\n",
       "        -0.0864,  0.0827,  0.1078,  0.0044, -0.0883, -0.1276, -0.0286, -0.0314],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in model.linear2.parameters()][0][:,0]\n",
    "[x for x in model.linear2.parameters()][0][:,1:]\n",
    "[x for x in model.linear2.parameters()][0][:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "07021713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1681,  0.2755, -0.1224, -0.2722,  0.6572,  0.1530,  0.1879, -0.2204,\n",
       "         0.6531, -0.3459,  0.0753, -0.3451,  0.4316, -0.1994,  0.1465,  0.3100,\n",
       "         0.0160, -0.3059,  0.4430,  0.1239, -0.5761,  0.0114, -0.2714,  0.5075,\n",
       "        -0.3019, -0.5241,  0.1346, -0.3772,  0.3538,  0.1654,  0.0055,  0.2951,\n",
       "         0.3105,  0.6046, -0.2313,  0.0493,  0.1271,  0.4579, -0.5893, -0.4037,\n",
       "         0.2082, -0.5880,  0.3097,  0.6044,  0.5200, -0.1111, -0.5425,  0.6961,\n",
       "        -0.6421,  0.5669], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in model.linear1.parameters()][0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "069e7b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8447e-05, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs = torch.rand(1000).reshape(-1,2)\n",
    "test_targets = test_inputs.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "F.mse_loss(model(test_inputs)[0],test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd2208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202283d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68b0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
